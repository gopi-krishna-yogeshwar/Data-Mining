{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from sklearn.metrics import classification_report\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.tsv\", usecols = ['review_body', 'star_rating'], sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['star_rating'] =  pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df = df.dropna(subset = ['star_rating'])\n",
    "df.loc[:,'star_rating'] = df['star_rating'].astype(int)\n",
    "df.loc[:,'review_body'] = df['review_body'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 20000 reviews randomly from each rating class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.DataFrame()\n",
    "for i in range(1,6):\n",
    "    #data_set.append()\n",
    "    temp = df[df['star_rating'] == i].sample(frac=1).reset_index(drop=True).iloc[:20000]\n",
    "    #print(type(temp))\n",
    "    data_set =data_set.append(temp, ignore_index=True)\n",
    "#print(len(data_set))   \n",
    "data_set = data_set.sample(frac=1).reset_index(drop=True)\n",
    "print(data_set.head()) \n",
    "global count\n",
    "count = 0\n",
    "data_set.to_csv(\"review_sample.csv\")\n",
    "\n",
    "def get_avg_length(data):\n",
    "    count = 0\n",
    "    for review in data['review_body']:\n",
    "        count += len(review)\n",
    "        \n",
    "    return count/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_set.head(20)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "word_list = brown.words()\n",
    "word_set = set(word_list)\n",
    "\n",
    "def print_metrics(report):\n",
    "    keys = ['1','2','3','4','5','macro avg']\n",
    "    result = \"\"\n",
    "    for key in keys:\n",
    "        p = \"precision:\" + str(report[key]['precision'])\n",
    "        r = \"recall:\" + str(report[key]['recall'])\n",
    "        f = \"f1-score:\" + str(report[key]['f1-score'])\n",
    "        row = \" , \".join([p,r,f])\n",
    "        result += key + \" - \" + row + \"\\n\"\n",
    "    return result\n",
    "\n",
    "def spell_correction(x) :\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    corrected_words = []\n",
    "    for word in word_tokenize(x) :\n",
    "        if word not in word_set:\n",
    "            new_word = spell.correction(word)\n",
    "            if new_word != None:\n",
    "                corrected_words.append(new_word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        else :\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "print(get_avg_length(data_set))\n",
    "#converting reviews to lower cases\n",
    "data_set['review_body'] = data_set['review_body'].str.lower()\n",
    "#extracting content in html tags\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : BeautifulSoup(x).get_text())\n",
    "#removing all urls in the review\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : re.sub(r'http\\S+','',x).strip())\n",
    "#expanding contraction words\n",
    "#data_set['review_body'] = data_set['review_body'].apply(lambda x : contractions.fix(x))\n",
    "#removing non alphabetical characters\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : \" \".join(re.sub('[^a-z]+','', word) for word in word_tokenize(x)))\n",
    "#removing extra spaces between words\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : re.sub(' +',' ',x).strip())\n",
    "data_set.head(20)\n",
    "print(get_avg_length(data_set))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#data_set['review_body'] = data_set['review_body'].apply(lambda x : \" \".join([token for token in x.split() if token not in stop_words])) \n",
    "print(get_avg_length(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "def lemmatize_words(sentence):\n",
    "    words_pos_tag = nltk.pos_tag(word_tokenize(sentence))\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), words_pos_tag))\n",
    "    final_words = []\n",
    "    for word,tag in wordnet_tagged:\n",
    "        if tag == None:\n",
    "            final_words.append(word)\n",
    "        else:\n",
    "            final_words.append(WordNetLemmatizer().lemmatize(word,tag))\n",
    "    return \" \".join(final_words)\n",
    "    \n",
    "\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda x : \" \".join([WordNetLemmatizer().lemmatize(token) for token in word_tokenize(x)]))\n",
    "start_time = time.time()\n",
    "#data_set['review_body'] = data_set['review_body'].apply(lambda x : lemmatize_words(x))\n",
    "print(time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_set.head(20))\n",
    "#data_set['review_body'] = data_set['review_body'].apply(lambda x : spell_correction(x))\n",
    "print(get_avg_length(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df = 5).fit(data_set['review_body'])\n",
    "tf_idf_transform = tf_idf_vectorizer.transform(data_set['review_body'])\n",
    "normalized_vector = StandardScaler(with_mean = False).fit_transform(tf_idf_transform)\n",
    "tf_idf_x_train,tf_idf_x_test,y_train,y_test = train_test_split(normalized_vector, data_set['star_rating'], test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tf_idf_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron_model = Perceptron()\n",
    "perceptron_model.fit(tf_idf_x_train, y_train)\n",
    "y_test_pred_perceptron = perceptron_model.predict(tf_idf_x_test)\n",
    "report = classification_report(y_test, y_test_pred_perceptron,output_dict=True)\n",
    "print(print_metrics(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(tf_idf_x_train, y_train)\n",
    "y_test_pred_svm = svm_classifier.predict(tf_idf_x_test)\n",
    "report=classification_report(y_test, y_test_pred_svm,output_dict=True)\n",
    "print(print_metrics(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(max_iter=800)\n",
    "lr_classifier.fit(tf_idf_x_train, y_train)\n",
    "y_test_pred_lr = lr_classifier.predict(tf_idf_x_test)\n",
    "report=classification_report(y_test, y_test_pred_lr,output_dict=True)\n",
    "print(print_metrics(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(tf_idf_x_train, y_train)\n",
    "y_test_pred_nb = nb_model.predict(tf_idf_x_test)\n",
    "report = classification_report(y_test, y_test_pred_nb,output_dict=True)\n",
    "print(print_metrics(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
